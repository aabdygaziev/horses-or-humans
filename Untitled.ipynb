{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import cv2 as cv\n",
    "import os\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'horse-or-human/train/'\n",
    "test_path = 'horse-or-human/validation/'\n",
    "categories = ['horses','humans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# horse images in train set:  500\n",
      "# human images in train set:  527\n",
      "# human images in test set:  128\n",
      "# human images in test set:  128\n"
     ]
    }
   ],
   "source": [
    "#sets\n",
    "print('# horse images in train set: ',len(os.listdir(train_path+categories[0])))\n",
    "print('# human images in train set: ',len(os.listdir(train_path+categories[1])))\n",
    "print('# human images in test set: ',len(os.listdir(test_path+categories[1])))\n",
    "print('# human images in test set: ',len(os.listdir(test_path+categories[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "horse = Image.open(train_path+categories[0]+'/horse01-0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "horse.size[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(train_path,categories[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv.imread(train_path+categories[0]+'/horse01-0.png',cv.IMREAD_COLOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to create train and test datasets\n",
    "def create_data(dataset_path):\n",
    "    data = []\n",
    "    for category in categories:\n",
    "        path = os.path.join(dataset_path,category)\n",
    "        class_number = categories.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            img_array = cv.imread(os.path.join(path,img),cv.IMREAD_COLOR)\n",
    "            data.append([img_array,class_number])\n",
    "    return data\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape:  (1027, 2)\n",
      "test data shape:  (256, 2)\n"
     ]
    }
   ],
   "source": [
    "# creating train,test numpy arrays\n",
    "train = create_data(train_path)\n",
    "test  = create_data(test_path)\n",
    "\n",
    "train = np.asarray(train)\n",
    "test  = np.asanyarray(test)\n",
    "\n",
    "print('train data shape: ',train.shape)\n",
    "print('test data shape: ',test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suffle the data\n",
    "random.shuffle(train)\n",
    "random.shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeling\n",
    "X_train = []\n",
    "y_train = []\n",
    "for feature,label in train:\n",
    "    X_train.append(feature)\n",
    "    y_train.append(label)\n",
    "    \n",
    "X_test = []\n",
    "y_test = []\n",
    "for feat, label in test:\n",
    "    X_test.append(feature)\n",
    "    y_test.append(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (1027, 300, 300, 3)\n",
      "y_train shape (1, 1027)\n",
      "X_test shape (256, 300, 300, 3)\n",
      "y_test shape (1, 256)\n"
     ]
    }
   ],
   "source": [
    "# converting intor numpy arrays\n",
    "X_train = np.asarray(X_train)\n",
    "X_test = np.asarray(X_test)\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "#reshaping labels\n",
    "y_train = np.reshape(y_train,newshape=(1,y_train.shape[0]))\n",
    "y_test = np.reshape(y_test,newshape=(1,y_test.shape[0]))\n",
    "    \n",
    "print('X_train shape', X_train.shape)\n",
    "print('y_train shape', y_train.shape)\n",
    "print('X_test shape', X_test.shape)\n",
    "print('y_test shape', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training examples:  1027\n",
      "# of testing examples:  256\n",
      "Height/Width of each image:  300\n"
     ]
    }
   ],
   "source": [
    "# number of training examples\n",
    "m_train = X_train.shape[0]\n",
    "# number of test examples\n",
    "m_test = X_test.shape[0]\n",
    "# Height/Width of each pciture\n",
    "n_pixel = X_train.shape[2]\n",
    "\n",
    "print('# of training examples: ',m_train)\n",
    "print('# of testing examples: ',m_test)\n",
    "print('Height/Width of each image: ',n_pixel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data new shape:  (270000, 1027)\n",
      "test data new shape:  (270000, 256)\n"
     ]
    }
   ],
   "source": [
    "# reshaping our dateset\n",
    "\n",
    "train_x_flatten = X_train.reshape(X_train.shape[0],-1).T\n",
    "test_x_flatten = X_test.reshape(X_test.shape[0],-1).T\n",
    "\n",
    "# dataset standardization\n",
    "train_x = train_flatten/255\n",
    "test_x = test_flatten/255\n",
    "\n",
    "print('train data new shape: ',train_x.shape)\n",
    "print('test data new shape: ',test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = np.random.randn(n_h,n_x)*0.01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h)*0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01624345 -0.00611756 -0.00528172]\n",
      " [-0.01072969  0.00865408 -0.02301539]]\n",
      "b1 = [[0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.01744812 -0.00761207]]\n",
      "b2 = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(3,2,1)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_deep\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = init_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    Z = np.dot(W,A)+b\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "    \n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A, cache = linear_activation_forward(A_prev,\n",
    "                                             parameters['W'+str(l)],\n",
    "                                             parameters['b'+str(l)],\n",
    "                                             activation='relu')\n",
    "        caches.append(cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    AL, cache = linear_activation_forward(A,\n",
    "                                          parameters['W'+str(L)],\n",
    "                                          parameters['b'+str(L)],\n",
    "                                          activation='sigmoid')\n",
    "    caches.append(cache)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    cost = np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))\n",
    "    cost = -cost/m\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    dW = (1/m) * np.dot(dZ,A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = relu_backward(dA, cache[1])\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = sigmoid_backward(dA, cache[1])\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    dAL = -(np.divide(Y,AL)-np.divide(1-Y,1-AL))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)]=linear_activation_backward(dAL, current_cache,activation='sigmoid')\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "    ### END CODE HERE ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization. (≈ 1 line of code)\n",
    "    ### START CODE HERE ###\n",
    "    parameters = init_parameters_deep(layers_dims)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        AL, caches = L_model_forward(X,parameters)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(AL,Y)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        grads = L_model_backward(AL,Y,caches)\n",
    "        ### END CODE HERE ###\n",
    " \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = 270000     # num_px * num_px * 3\n",
    "n_h = 7\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = X.shape[1]                           # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    parameters = init_params(n_x,n_h,n_y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A1, cache1 = linear_activation_forward(X,W1,b1,activation='relu')\n",
    "        A2, cache2 = linear_activation_forward(A1,W2,b2,activation='sigmoid')\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(A2,Y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2,cache2,'sigmoid')\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1,cache1,'relu')\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (approx. 1 line of code)\n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.7080010095433406\n",
      "Cost after iteration 100: 0.41642694053034435\n",
      "Cost after iteration 200: 0.396469897141505\n",
      "Cost after iteration 300: 0.39873757924274805\n",
      "Cost after iteration 400: 0.3876114415410953\n",
      "Cost after iteration 500: 0.37742909187685725\n",
      "Cost after iteration 600: 0.3743001594271984\n",
      "Cost after iteration 700: 0.3567770998872938\n",
      "Cost after iteration 800: 0.3547576382712093\n",
      "Cost after iteration 900: 0.3322354842576073\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = two_layer_model(train_x, y_train, layers_dims = (n_x, n_h, n_y), num_iterations = 1000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
